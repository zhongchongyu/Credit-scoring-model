{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'D:/DataSet/Credit/'\n",
    "train = pd.read_csv(base_path + 'featured/train_all_feature_log.csv')\n",
    "test = pd.read_csv(base_path + 'featured/test_all_feature_log.csv')\n",
    "\n",
    "one_hot = 0\n",
    "if one_hot:\n",
    "    train = pd.get_dummies(train)\n",
    "    test = pd.get_dummies(test)\n",
    "    \n",
    "col_to_drop = [\n",
    "    # 1\n",
    "    'count_house_loan_ln',# 0.97232912500359225)\n",
    "    'count_house_loan',# 0.65395611105534779)\n",
    "    'edu_level_other',# 0.57350198625873861)\n",
    "    'count_payment_state_E_ln',# 0.51044888053676218)\n",
    "    'count_attention_ln',# 0.32350891467937332)\n",
    "    'count_commercial_loan',# 0.0)\n",
    "    'count_sixty_ovd_dw',# 0.0)\n",
    "    'count_sixty_ovd_months',# 0.0)\n",
    "    'ind_sixty_max_duration',# 0.0)\n",
    "    'marry_status_other',# 0.0)\n",
    "    'count_study_loan_ln',# 0.0)\n",
    "    'count_housing_accumulation_ln',# 0.0)\n",
    "    'count_commercial_housing_ln',# 0.0)\n",
    "    'count_combination_ensure_ln',# 0.0)\n",
    "    'ind_other_counts_lnd',# 0.0)\n",
    "    'count_combination_lnd',# 0.0)\n",
    "    'count_pledge_guarantee_lnd',# 0.0)\n",
    "    'count_ensure_lnd',# 0.0)\n",
    "    'count_other_guarantee_lnd',# 0.0)\n",
    "    'count_combination_ensure_lnd',# 0.0)\n",
    "    'count_farmer_joint_lnd',# 0.0)\n",
    "    'count_pledge_guarantee_bail_lnd',# 0.0)\n",
    "    # 10\n",
    "    'ind_curr_overdue_cyc_lnd',# 9.1747017468829988)\n",
    "    'ind_unact_counts_lnd',# 8.1829607049772104)\n",
    "    'ind_other_counts',# 8.1190294047611253)\n",
    "    'count_pledge_guarantee_bail_ln',# 4.9866824922417035)\n",
    "    'count_sharedebt',# 4.25470982342895)\n",
    "    'marry_status_unmarried',# 3.5268450817329375)\n",
    "    'count_normal_ln',# 3.1544560288384238)\n",
    "    'count_farmer_joint_ln',# 2.8250789765495536)\n",
    "    'not_clear_account_count',# 2.438362051462843)\n",
    "    'count_car_loan_ln',# 2.0244134983035815)\n",
    "    'count_payment_state_E_lnd',# 1.9511014375106948)\n",
    "    # 40\n",
    "    'ind_normal_counts',# 38.981578298058366)\n",
    "    'count_ensure_ln',# 32.686012781155711)\n",
    "    'not_logout_pre_account_count',# 29.035934862113912)\n",
    "    'count_spl',# 26.621401271906876)\n",
    "    'not_logout_pre_finance_org_count',# 20.88833411697636)\n",
    "    'count_debit_card_ovd_dw',# 20.552771641255653)\n",
    "    'ind_clear_counts_lnd',# 18.63633007759838)\n",
    "    'cat_query_reason_mal',# 18.012892273855371)\n",
    "    'count_other_guarantee_ln',# 15.719418555370231)\n",
    "    'count_pledge_guarantee_ln',# 14.603172531948651)\n",
    "    'count_standard_loancard',# 14.257967916620288)\n",
    "    'count_combination_ln',# 13.225866749150008)\n",
    "    'marry_status_divorced',# 12.996673186745294)\n",
    "    'flt_highest_sixty_oa_per_mon',# 12.496437876453976)\n",
    "    'count_farmer_loan_ln',# 12.034259912867824)\n",
    "    # 60\n",
    "    'curr_overdue_cyc_days',# 58.533138767951407)\n",
    "    'not_logout_pre_max_credit_limit_per_org',# 55.545770066417433)\n",
    "    'not_logout_pre_finance_corp_count',# 54.909995520093744)\n",
    "    'has_fund',# 53.524477466077244)\n",
    "    'edu_level_bachelor',# 51.681921368379456)\n",
    "    'cat_query_reason_sqe',# 50.507370364531575)\n",
    "    'not_clear_finance_org_count',# 50.499752342403504)\n",
    "    'count_payment_state_B_ln',# 49.675949679726415)\n",
    "    'count_ovd',# 47.284280675138668)\n",
    "    'marry_status_married',# 47.141063732728689)\n",
    "    # 100\n",
    "    'count_payment_state_D_ln',# 98.582591891500016)\n",
    "    'not_logout_finance_org_count',# 97.63379220401383)\n",
    "    'count_credit_loan_ln',# 96.057688729626022)\n",
    "    'count_payment_state_D_lnd',# 94.52677681813546)\n",
    "    'all_highest_oa_per_mon',# 93.758390716396264)\n",
    "    'count_consumption loan_ln',# 92.34403542313882)\n",
    "    'not_logout_pre_latest_6m_used_avg_amount',# 85.357040829001591)\n",
    "    'balance',# 85.186190705260103)\n",
    "    'cat_query_reason_la',# 83.737758346562558)\n",
    "    'ind_clear_counts',# 82.467554392775014)\n",
    "    'count_unknown_ln',# 73.400641666623201)\n",
    "    'not_logout_pre_min_credit_limit_per_org',# 70.752886069309909)\n",
    "    'not_logout_pre_credit_limit',# 68.394379619115668)\n",
    "    'not_logout_account_count',# 67.926934887483739)\n",
    "    'lnd_ovd_sum_amount',# 67.598474433716945)\n",
    "    'flt_highest_debit_card_oa_per_mon',# 64.9057845815354)\n",
    "    'count_loan_ovd_dw',# 64.132474960621153)\n",
    "    'count_operating_loan_ln',# 63.127772355688421)\n",
    "    'not_clear_finance_corp_count',# 62.877948676798582)\n",
    "    # 180\n",
    "    'not_logout_latest_6m_used_avg_amount',# 176.32317263727683)\n",
    "    'ind_loan_max_duration',# 167.56724876084013)\n",
    "    'count_credit_loan_lnd',# 166.96045957856194)\n",
    "    'all_max_duration',# 153.57957111737943)\n",
    "    'not_clear_balance',# 153.18548993022915)\n",
    "    'flt_highest_loan_oa_per_mon',# 150.5264464131252)\n",
    "    'flt_sum_amount',# 149.83162201460962)\n",
    "    'used_highest_amount_lnd',# 136.85598322859255)\n",
    "    'scheduled_payment_amount',# 128.02030046387119)\n",
    "    'latest6_month_used_avg_amount_lnd',# 124.58135686300396)\n",
    "    'ind_org_counts',# 120.5490321578975)\n",
    "    'ind_debit_card_max_duration',# 118.53218655429413)\n",
    "    'edu_level_junior',# 117.02178169627837)\n",
    "    'lnd_ovd_sum_last_months',# 114.74345491704378)\n",
    "    'used_credit_limit_amount_lnd',# 114.2672848868574)\n",
    "    'ave_ovd_amount',# 114.06571798355444)\n",
    "    'count_other_loan',# 111.82596886962884)\n",
    "    'range_lnd_ovd',# 109.13363916690426)\n",
    "    'not_logout_pre_used_credit_limit',# 105.64340446411771)\n",
    "    'count_sum_ovd_dw',# 104.97563024241767)\n",
    "    # flt_noise\n",
    "    'not_logout_max_credit_limit_per_org',# 211.80030432198566)\n",
    "    # 240\n",
    "    #'not_logout_finance_corp_count',# 217.97120979038462)\n",
    "]\n",
    "\n",
    "train = train.drop(col_to_drop, axis=1)\n",
    "test = test.drop(col_to_drop,axis=1)\n",
    "test.insert(0,'y',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat([train,test])\n",
    "x = x.reset_index(drop=True)\n",
    "#unwanted = x.columns[x.columns.str.startswith('ps_calc_')]\n",
    "#x.drop(unwanted,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1        28\n",
       "2        26\n",
       "3         0\n",
       "4        20\n",
       "5        20\n",
       "6        24\n",
       "7        29\n",
       "8        23\n",
       "9        25\n",
       "10       25\n",
       "11       18\n",
       "12        0\n",
       "13       36\n",
       "14       28\n",
       "15        0\n",
       "16       26\n",
       "17        0\n",
       "18       31\n",
       "19       30\n",
       "20        0\n",
       "21       35\n",
       "22       28\n",
       "23       18\n",
       "24       27\n",
       "25       32\n",
       "26       19\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "         ..\n",
       "39970    29\n",
       "39971    35\n",
       "39972    34\n",
       "39973    29\n",
       "39974    34\n",
       "39975    26\n",
       "39976    33\n",
       "39977    30\n",
       "39978    24\n",
       "39979     0\n",
       "39980    21\n",
       "39981    31\n",
       "39982    30\n",
       "39983    26\n",
       "39984    30\n",
       "39985    30\n",
       "39986    27\n",
       "39987    32\n",
       "39988    32\n",
       "39989    31\n",
       "39990    17\n",
       "39991    26\n",
       "39992    29\n",
       "39993    33\n",
       "39994    29\n",
       "39995     0\n",
       "39996    29\n",
       "39997    32\n",
       "39998    31\n",
       "39999    24\n",
       "Name: count_payment_state_A_ln, Length: 40000, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train.count_payment_state_A_ln\n",
    "pd.cut(x['count_payment_state_A_ln'], 50,labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent 2 2\n",
      "is_local 2 2\n",
      "work_province 285 313\n",
      "salary 7 7\n",
      "count_loancard 60 50\n",
      "count_other_loan_ln 47 39\n",
      "count_payment_state_A_ln 586 510\n",
      "count_payment_state_C_ln 219 181\n",
      "credit_limit_amount 16270 7191\n",
      "remain_payment_cyc_days 950 700\n",
      "actual_payment_amount 8192 5257\n",
      "curr_overdue_amount 166 24\n",
      "ind_normal_counts_lnd 119 96\n",
      "count_payment_state_A_lnd 327 310\n",
      "count_payment_state_B_lnd 393 338\n",
      "count_payment_state_C_lnd 344 304\n",
      "credit_limit_amount_lnd 20104 7313\n",
      "share_credit_limit_amount_lnd 7774 3463\n",
      "scheduled_payment_amount_lnd 12636 6798\n",
      "actual_payment_amount_lnd 18822 7956\n",
      "curr_overdue_amount_lnd 297 135\n",
      "not_clear_credit_limit 12054 5706\n",
      "not_clear_latest_6m_used_avg_amount 6870 4683\n",
      "not_logout_finance_corp_count 17 16\n",
      "not_logout_credit_limit 6985 3203\n",
      "not_logout_min_credit_limit_per_org 1694 813\n",
      "not_logout_used_credit_limit 24153 9158\n",
      "count_sum_ovd_months 74 63\n",
      "count_loan_ovd_months 47 37\n",
      "count_debit_card_ovd_months 59 53\n",
      "count_month 197 134\n",
      "ind_query_reason_0 59 54\n",
      "ind_query_reason_1 47 50\n",
      "ind_query_reason_2 18 16\n",
      "ind_query_reason_sum 84 83\n",
      "edu_level_middle 2 2\n",
      "edu_level_under 2 2\n"
     ]
    }
   ],
   "source": [
    "features = x.columns[2:]\n",
    "categories = []\n",
    "for c in features:\n",
    "    trainno = len(x.loc[:train.shape[0],c].unique())\n",
    "    testno = len(x.loc[train.shape[0]:,c].unique())\n",
    "    print(c,trainno,testno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for each in x.columns[2:]:\n",
    "    if len(x.loc[:,each].unique()) <= 2:\n",
    "        pass\n",
    "    \n",
    "    elif len(x.loc[:,each].unique()) <= 50:\n",
    "        x.loc[:,each] =  pd.cut(x[each], 4,labels=False)\n",
    "        \n",
    "    elif len(x.loc[:,each].unique()) <= 500:\n",
    "        x.loc[:,each] =  pd.cut(x[each], 12,labels=False)\n",
    "        \n",
    "    else :\n",
    "        x.loc[:,each] =  pd.cut(x[each], 50,labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x.loc[train.shape[0]:].copy()\n",
    "train = x.loc[:train.shape[0]-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always good to shuffle for SGD type optimizers\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_id = train.copy()\n",
    "train.drop('report_id',inplace=True,axis=1)\n",
    "test.drop('report_id',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 0\n",
      "Row 5000\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "folds = StratifiedKFold(n_splits=K, shuffle=True, random_state=99) \n",
    "for folds, (train_index, valid_index) in enumerate(folds.split(train.y,train.y)):\n",
    "    \n",
    "    fold_train, fold_valid = train.iloc[train_index,:].copy(), train.iloc[valid_index,:].copy()\n",
    "    \n",
    "    fold_train_id, fold_valid_id = train_with_id.iloc[train_index,:].copy(), train_with_id.iloc[valid_index,:].copy()\n",
    "    fold_train_id.to_csv('fold'+str(folds)+'_train.csv', float_format='%.6f', index=False)\n",
    "    fold_valid_id.to_csv('fold'+str(folds)+'_valid.csv', float_format='%.6f', index=False)\n",
    "    \n",
    "    \n",
    "    ########## 训练集与测试集 ###########\n",
    "    categories = train.columns[1:]  # 离散型变量集合\n",
    "    numerics = []                   # 连续型变量集合\n",
    "    currentcode = len(numerics)     # 连续性变量数量\n",
    "    catdict = {}                    # 表示每个变量是否是离散的 bin 字典\n",
    "    catcodes = {}\n",
    "    for x in numerics:\n",
    "        catdict[x] = 0              # 表示该特征不是离散型\n",
    "    for x in categories:\n",
    "        catdict[x] = 1              # 表示该特征是离散型\n",
    "\n",
    "    noofrows = fold_train.shape[0]\n",
    "    noofcolumns = len(features)\n",
    "    with open(\"fold\"+str(folds)+\"_trainffm_T.txt\", \"w\") as train_file:\n",
    "        for n, r in enumerate(range(noofrows)):\n",
    "            if((n%5000)==0):\n",
    "                print('Row',n)\n",
    "            datastring = \"\"\n",
    "            datarow = fold_train.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "            datastring += '\\n'\n",
    "            train_file.write(datastring)\n",
    "            \n",
    "    noofrows = test.shape[0]\n",
    "    noofcolumns = len(features)\n",
    "    with open(\"alltestffm\"+str(folds)+\".txt\", \"w\") as text_file:\n",
    "        for n, r in enumerate(range(noofrows)):\n",
    "            if((n%5000)==0):\n",
    "                print('Row',n)\n",
    "            datastring = \"\"\n",
    "            datarow = test.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "            datastring += '\\n'\n",
    "            text_file.write(datastring)\n",
    "            \n",
    "    ######## 训练集与验证集 ###########        \n",
    "    categories = train.columns[1:]  # 离散型变量集合\n",
    "    numerics = []                   # 连续型变量集合\n",
    "    currentcode = len(numerics)     # 连续性变量数量\n",
    "    catdict = {}                    # 表示每个变量是否是离散的 bin 字典\n",
    "    catcodes = {}\n",
    "    for x in numerics:\n",
    "        catdict[x] = 0              # 表示该特征不是离散型\n",
    "    for x in categories:\n",
    "        catdict[x] = 1              # 表示该特征是离散型\n",
    "\n",
    "    noofrows = fold_train.shape[0]\n",
    "    noofcolumns = len(features)\n",
    "    with open(\"fold\"+str(folds)+\"_trainffm_V.txt\", \"w\") as train_file:\n",
    "        for n, r in enumerate(range(noofrows)):\n",
    "            if((n%5000)==0):\n",
    "                print('Row',n)\n",
    "            datastring = \"\"\n",
    "            datarow = fold_train.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "            datastring += '\\n'\n",
    "            train_file.write(datastring)\n",
    "            \n",
    "    noofrows = fold_valid.shape[0]\n",
    "    noofcolumns = len(features)\n",
    "    with open(\"fold\"+str(folds)+\"_validffm.txt\", \"w\") as valid_file:\n",
    "        for n, r in enumerate(range(noofrows)):\n",
    "            if((n%5000)==0):\n",
    "                print('Row',n)\n",
    "            datastring = \"\"\n",
    "            datarow = fold_valid.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "            datastring += '\\n'\n",
    "            valid_file.write(datastring)\n",
    "    \n",
    "    \n",
    "    '''noofrows = train.shape[0]\n",
    "    noofcolumns = len(features)\n",
    "    with open(\"alltrainffm\"+str(folds)+\".txt\", \"w\") as train_file:\n",
    "        for n, r in enumerate(range(noofrows)):\n",
    "            if((n%5000)==0):\n",
    "                print('Row',n)\n",
    "            datastring = \"\"\n",
    "            datarow = train.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "            datastring += '\\n'\n",
    "            train_file.write(datastring)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0\n",
      "Row 5000\n",
      "Row 10000\n",
      "Row 15000\n",
      "Row 20000\n",
      "Row 25000\n",
      "Row 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1dfd110aeea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mdatastring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatastring\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\":1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mdatastring\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatastring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "categories = train.columns[1:]  # 离散型变量集合\n",
    "numerics = []                   # 连续型变量集合\n",
    "\n",
    "currentcode = len(numerics)     # 连续性变量数量\n",
    "catdict = {}                    # 表示每个变量是否是离散的 bin 字典\n",
    "catcodes = {}\n",
    "\n",
    "for x in numerics:\n",
    "    catdict[x] = 0              # 表示该特征不是离散型\n",
    "for x in categories:\n",
    "    catdict[x] = 1              # 表示该特征是离散型\n",
    "\n",
    "\n",
    "noofrows = train.shape[0]\n",
    "noofcolumns = len(features)\n",
    "with open(\"alltrainffm.txt\", \"w\") as train_file:\n",
    "    for n, r in enumerate(range(noofrows)):\n",
    "        if((n%5000)==0):\n",
    "            print('Row',n)\n",
    "        datastring = \"\"\n",
    "        datarow = train.iloc[r].to_dict()\n",
    "        datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "        for i, x in enumerate(catdict.keys()):\n",
    "            if(catdict[x]==0):\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "            else:\n",
    "                if(x not in catcodes):\n",
    "                    catcodes[x] = {}\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "                elif(datarow[x] not in catcodes[x]):\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                code = catcodes[x][datarow[x]]\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "        datastring += '\\n'\n",
    "        train_file.write(datastring)\n",
    "        \n",
    "\n",
    "noofrows = test.shape[0]\n",
    "noofcolumns = len(features)\n",
    "with open(\"alltestffm.txt\", \"w\") as text_file:\n",
    "    for n, r in enumerate(range(noofrows)):\n",
    "        if((n%5000)==0):\n",
    "            print('Row',n)\n",
    "        datastring = \"\"\n",
    "        datarow = test.iloc[r].to_dict()\n",
    "        datastring += str(int(datarow['y']))\n",
    "\n",
    "\n",
    "        for i, x in enumerate(catdict.keys()):\n",
    "            if(catdict[x]==0):\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "            else:\n",
    "                if(x not in catcodes):\n",
    "                    catcodes[x] = {}\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "                elif(datarow[x] not in catcodes[x]):\n",
    "                    currentcode +=1\n",
    "                    catcodes[x][datarow[x]] = currentcode\n",
    "\n",
    "                code = catcodes[x][datarow[x]]\n",
    "                datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "        datastring += '\\n'\n",
    "        text_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Gini:\n",
      "0.698537851259\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "### 计算 gini 并将预测结果转化成 csv ###\n",
    "########################################\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv( 'D:/DataSet/Credit/featured/train_all_feature.csv')\n",
    "test = pd.read_csv( 'D:/DataSet/Credit/featured/test_all_feature.csv')\n",
    "\n",
    "base_path = 'D:/DataSet/Credit/libffm/'\n",
    "sub_train_0 = pd.read_csv(base_path+'fold0/fold0_valid.csv')\n",
    "sub_train_1 = pd.read_csv(base_path+'fold1/fold1_valid.csv')\n",
    "sub_train_2 = pd.read_csv(base_path+'fold2/fold2_valid.csv')\n",
    "sub_train_3 = pd.read_csv(base_path+'fold3/fold3_valid.csv')\n",
    "sub_train_4 = pd.read_csv(base_path+'fold4/fold4_valid.csv')\n",
    "\n",
    "pred_train_0 = pd.read_csv(base_path+'output_valid_0',header=None)\n",
    "pred_train_0.columns = ['pred']\n",
    "pred_train_1 = pd.read_csv(base_path+'output_valid_1',header=None)\n",
    "pred_train_1.columns = ['pred']\n",
    "pred_train_2 = pd.read_csv(base_path+'output_valid_2',header=None)\n",
    "pred_train_2.columns = ['pred']\n",
    "pred_train_3 = pd.read_csv(base_path+'output_valid_3',header=None)\n",
    "pred_train_3.columns = ['pred']\n",
    "pred_train_4 = pd.read_csv(base_path+'output_valid_4',header=None)\n",
    "pred_train_4.columns = ['pred']\n",
    "\n",
    "all_train = pd.concat([sub_train_0,sub_train_1,sub_train_2,sub_train_3,sub_train_4],axis=0)\n",
    "all_pred = pd.concat([pred_train_0,pred_train_1,pred_train_2,pred_train_3,pred_train_4],axis=0)\n",
    "sub_train = pd.DataFrame()\n",
    "sub_train['report_id'] = all_train.report_id\n",
    "sub_train['y'] = all_train.y\n",
    "sub_train['pred'] = all_pred.pred\n",
    "sub_train = sub_train.sort_values('report_id')\n",
    "\n",
    "\n",
    "pred_test_0 = pd.read_csv(base_path+'output_test_0',header=None)\n",
    "pred_test_1 = pd.read_csv(base_path+'output_test_1',header=None)\n",
    "pred_test_2 = pd.read_csv(base_path+'output_test_2',header=None)\n",
    "pred_test_3 = pd.read_csv(base_path+'output_test_3',header=None)\n",
    "pred_test_4 = pd.read_csv(base_path+'output_test_4',header=None)\n",
    "pre_ave_test = (pred_test_0 + pred_test_1 + pred_test_2 + pred_test_3 + pred_test_4) / 5\n",
    "pre_ave_test.columns = ['pred']\n",
    "sub = pd.DataFrame()\n",
    "sub['report_id'] = test.report_id\n",
    "sub['pred'] = pre_ave_test.pred\n",
    "\n",
    "print('OOF Gini:')\n",
    "print(eval_gini(sub_train.y, sub_train.pred))\n",
    "\n",
    "sub_train.to_csv('D:/DataSet/Credit/result/FFM_train.csv', index=False)\n",
    "sub.to_csv('D:/DataSet/Credit/result/FFM_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706538666667\n",
      "0.685101985185\n",
      "0.670442666667\n",
      "0.70998802963\n",
      "0.724904296296\n"
     ]
    }
   ],
   "source": [
    "print(eval_gini(sub_train_0.y, pred_train_0.pred))\n",
    "print(eval_gini(sub_train_1.y, pred_train_1.pred))\n",
    "print(eval_gini(sub_train_2.y, pred_train_2.pred))\n",
    "print(eval_gini(sub_train_3.y, pred_train_3.pred))\n",
    "print(eval_gini(sub_train_4.y, pred_train_4.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706538666667\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "### 计算 gini 并将预测结果转化成 csv ###\n",
    "########################################\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv( 'D:/DataSet/Credit/featured/train_all_feature.csv')\n",
    "test = pd.read_csv( 'D:/DataSet/Credit/featured/test_all_feature.csv')\n",
    "\n",
    "base_path = 'D:/DataSet/Credit/libffm/'\n",
    "sub_train_0 = pd.read_csv(base_path+'fold0/fold0_valid.csv')\n",
    "pred_train_0 = pd.read_csv(base_path+'output_valid_0',header=None)\n",
    "pred_train_0.columns = ['pred']\n",
    "print(eval_gini(sub_train_0.y, pred_train_0.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
